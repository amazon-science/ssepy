{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle efficiency computations\n",
    "\n",
    "If you had to estimate a metric on a dataset of size $N$ but could only observe the labels of a data subset of size $n$, how should you strategically select this subset to obtain the most precise estimate of the metric?\n",
    "\n",
    "The goal of this notebook is to measure the precision of our estimates of a metric of interest $\\mathbb{E}[Z]$ across sampling methods and estimators on a given dataset (see the paper for details on the notation). We analyze:\n",
    "* Estimators: Horvitz-Thompson (HT) and difference (DF) estimators\n",
    "* Sample designs: simple random sampling (SRS) and stratified simple random sampling (SSRS) with proportional and Neyman allocation, all without replacement\n",
    "\n",
    "For this benchmarking exercise, we will assume that we have access to all labels $Z$. This will allow us to compute the exact risk of the estimators, which otherwise would not be possible. \n",
    "We will focus on estimating the precision of the accuracy estimates of a multi-class classifier. Other metrics can be estimated in a similar way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a multi-class classification task on Caltech 101. Predictions are generated by a CLIP model with ResNet 50 as visual encoder. Let's load packages as well as predictions $(m_1(X), \\dots, m_K(X))$ and ground truth labels ($Y$). You can also plug in your own data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cascade import OracleEstimator, ModelPerformanceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/predictions/\"\n",
    "data_name = \"caltech101/RN50_zero_shot.pt\"\n",
    "df = torch.load(data_folder + data_name)\n",
    "\n",
    "labels, preds = np.array(df[\"Target\"]), np.array(df[\"PredictionProb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the values of the performance metric, $Z$. Here we take $Z$ to be the accuracy of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(preds, axis=1)\n",
    "performance = (predicted_labels == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict performance\n",
    "\n",
    "We obtain an estimate of the expected performance for each observation, that is we construct a proxy $\\hat{Z}$ for $Z$. This proxy can be based on _anything_, but, the more strongly associated it is with $Z$, the more precise our estimates of $\\mathbb{E}[Z]$ will be. \n",
    "\n",
    "Here we use the predictions of the model under evaluation to construct $\\hat{Z}$. This means that we set $\\hat{Z} = \\argmax_{k\\in [K]} m_k(X)$. Ideally, you would want to at least calibrate these predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_performance =  preds[torch.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratify\n",
    "\n",
    "SSRS requires dividing the population into strata, from which we will draw the observations. \n",
    "We form the strata by running k-means on the predictions, following the recommendations from the paper. However, other methods can be applied here as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = OracleEstimator(performance=performance, proxy_performance=proxy_performance, total_samples=len(proxy_performance), budget=100)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "oracle.evaluator.stratify_data(KMeans(n_clusters=10, random_state=0, n_init=\"auto\"), oracle.proxy_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the variance of the sample design+estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compute the variance of our estimators. \n",
    "To compute the variance of the DF estimator instead of HT, we simply need to pass `outcomes-predictions` to the functions above instead of `outcomes`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sampling_variances(oracle):\n",
    "    \n",
    "    estimators = {\n",
    "        \"HT\": performance,\n",
    "        \"DF\": performance - proxy_performance\n",
    "    }\n",
    "    \n",
    "    strata_labels = oracle.evaluator.strata_labels\n",
    "    \n",
    "    results = {}\n",
    "    for est, values in estimators.items():\n",
    "        results[est] = {}\n",
    "\n",
    "         \n",
    "        results[est]['SRS'] = oracle.get_srs_variance(outcomes=values)\n",
    "        \n",
    "        oracle.evaluator.allocate_budget(allocation_type=\"proportional\")\n",
    "        results[est]['SSRS proportional'] = oracle.get_ssrs_proportional_variance(outcomes=values)\n",
    "        \n",
    "        oracle.evaluator.allocate_budget(variances_by_strata=[\n",
    "            np.mean(oracle.proxy_performance[oracle.evaluator.strata_labels == s]) * (1 - np.mean(oracle.proxy_performance[oracle.evaluator.strata_labels == s]))\n",
    "            for s in np.unique(oracle.evaluator.strata_labels)\n",
    "        ], allocation_type=\"neyman\")\n",
    "        results[est]['SSRS Neyman'] = oracle.get_ssrs_neyman_variance(outcomes=values)\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient=\"index\").T  \n",
    "\n",
    "results = calculate_sampling_variances(oracle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the variances of the estimators under the different sampling designs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT</th>\n",
       "      <th>DF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRS</th>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSRS proportional</th>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSRS Neyman</th>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.002095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         HT        DF\n",
       "SRS                0.002398  0.001711\n",
       "SSRS proportional  0.001611  0.001606\n",
       "SSRS Neyman        0.002110  0.002095"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "error-estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
