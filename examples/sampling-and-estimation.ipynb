{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "How would you choose $n$ observations from a total of $N$ to effectively estimate (say) the accuracy of a classifier? For example, imagine that our budget is limited and we can only annotate $n=100$ examples from data of size $N=10^{7}$! \n",
    "\n",
    "In this notebook, we show how to \n",
    "* Sample with stratified simple random sampling (SSRS) with proportional allocation\n",
    "* Estimate the metric of interest $\\mathbb{E}[Z]$ with the Horvitz-Thompson (HT) and difference (DF) estimators\n",
    "\n",
    "Besides estimating the value of the metric, we also computs its variance, which would allow us to create confidence intervals for the estimates.  \n",
    "\n",
    "We focus on estimating the precision of the binary accuracy of a multi-class classifier. Other evaluation metrics can be estimated in a similar way. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Consider a multi-class classification task. Let's load the packages as well as predictions $(m_1(X), \\dots, m_K(X))$ because that's all we have right now. You can also plug in your own data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ssepy import ModelPerformanceEvaluator\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "num_samples = 1000  # Total population size\n",
    "num_classes = 10    # Number of prediction classes\n",
    "num_features = 20   # Features for clustering\n",
    "\n",
    "# Generate random features and predictions\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Random features (normal distribution)\n",
    "features = np.random.randn(num_samples, num_features)\n",
    "preds = np.random.rand(num_samples, num_classes)\n",
    "preds = np.exp(preds) / np.sum(np.exp(preds), axis=1, keepdims=True)\n",
    "\n",
    "budget = 100\n",
    "total_sample_size = num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take performance to be the binary accuracy of the classifier and we will try to estimate its value on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict performance\n",
    "\n",
    "We obtain an estimate of the expected performance for each observation, that is we construct a proxy $\\hat{Z}$ for $Z$. This proxy can be based on _anything_, but, the more strongly associated it is with $Z$, the more precise our estimates of $\\mathbb{E}[Z]$ will be. \n",
    "\n",
    "In this notebook we use the predictions of the model under evaluation to construct $\\hat{Z}$. This means that we set $\\hat{Z} = \\arg \\max_{k\\in [K]} m_k(X)$. Ideally, we may want to at least calibrate these predictions. Let's skip this step here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create proxy performance measures\n",
    "ground_truth = np.random.randint(0, num_classes, size=num_samples)\n",
    "predicted_labels = np.argmax(preds, axis=1)\n",
    "proxy_performance = preds[np.arange(num_samples), predicted_labels]\n",
    "performance = (predicted_labels == ground_truth)\n",
    "\n",
    "evaluator = ModelPerformanceEvaluator(\n",
    "    Yh=proxy_performance,\n",
    "    budget=budget,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stratify\n",
    "\n",
    "SSRS requires dividing the population into strata, from which we will select which samples should be annotated. We form the strata by running k-means on the predictions, following the recommendations from the paper. However, other sample designs can be applied here as well, e.g., the strata could be formed by running a Gaussian mixture model on the feature representations of the data obtained from a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.stratify_data(clustering_algo = KMeans(n_clusters=5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample\n",
    "\n",
    "We now sample from the data with SSRS with proportional allocation. In practice, you would choose only strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize performance evaluator with synthetic data\n",
    "evaluator.allocate_budget(allocation_type=\"proportional\")\n",
    "sampled_idx = evaluator.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Annotate\n",
    "\n",
    "Pretend that in this step we annotate the selected samples. Here they are (luckily) already available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Estimate\n",
    "\n",
    "Now we can estimate the performance on our data subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is  [0.12947919]\n",
      "Variance is  [0.00100167]\n",
      "Mean is  [0.13163876]\n",
      "Variance is  [0.00100172]\n"
     ]
    }
   ],
   "source": [
    "estimates_ht = evaluator.compute_estimate(performance[sampled_idx])\n",
    "print('Mean is ', estimates_ht[0])\n",
    "print('Variance is ', estimates_ht[1])\n",
    "\n",
    "# for the difference estimator\n",
    "estimates_df = evaluator.compute_estimate(performance[sampled_idx], estimator = \"df\")\n",
    "print('Mean is ', estimates_df[0])\n",
    "print('Variance is ', estimates_df[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
