{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "How would you choose $n$ observations from a total of $N$ to effectively estimate (say) the accuracy of a classifier? For example, imagine that our budget is limited and we can only annotate $n=100$ examples from data of size $N=10^{7}$! \n",
    "\n",
    "In this notebook, we show how to \n",
    "* Sample via simple random sampling (SRS) and stratified simple random sampling (SSRS) with proportional and Neyman allocation, all without replacement\n",
    "* Estimate the metric of interest $\\mathbb{E}[Z]$ with the Horvitz-Thompson (HT) and difference (DF) estimators\n",
    "\n",
    "Besides estimating the value of the metric, we also computs its variance, which would allow us to create confidence intervals for the estimates.  \n",
    "\n",
    "We focus on estimating the precision of the binary accuracy of a multi-class classifier. Other evaluation metrics can be estimated in a similar way. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Consider a multi-class classification task on ImageNet-A. Predictions are generated by a CLIP model with ViT-L-14 as visual encoder. Let's load the packages as well as predictions $(m_1(X), \\dots, m_K(X))$ because that's all we have right now. You can also plug in your own data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cascade import ModelPerformanceEvaluator\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/predictions/\"\n",
    "data_name = \"imagenet-a/ViT-L-14_zero_shot.pt\"\n",
    "df = torch.load(data_folder + data_name)\n",
    "\n",
    "preds = np.array(df[\"PredictionProb\"]) # model predictions\n",
    "budget = 100 # sample size survey\n",
    "total_sample_size = len(preds) # total siperformancee of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take performance to be the binary accuracy of the classifier and we will try to estimate its value on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict performance\n",
    "\n",
    "We obtain an estimate of the expected performance for each observation, that is we construct a proxy $\\hat{Z}$ for $Z$. This proxy can be based on _anything_, but, the more strongly associated it is with $Z$, the more precise our estimates of $\\mathbb{E}[Z]$ will be. \n",
    "\n",
    "In this notebook we use the predictions of the model under evaluation to construct $\\hat{Z}$. This means that we set $\\hat{Z} = \\arg \\max_{k\\in [K]} m_k(X)$. Ideally, we may want to at least calibrate these predictions. Let's skip this step here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(preds, axis=1)\n",
    "proxy_performance = preds[np.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stratify\n",
    "\n",
    "SSRS requires dividing the population into strata, from which we will select which samples should be annotated. We form the strata by running k-means on the predictions, following the recommendations from the paper. However, other sample designs can be applied here as well, e.g., the strata could be formed by running a Gaussian mixture model on the feature representations of the data obtained from a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelPerformanceEvaluator(proxy_performance=proxy_performance, budget = budget)\n",
    "evaluator.stratify_data(features=proxy_performance, clustering_algorithm=KMeans(n_clusters=10, random_state=0, n_init='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample\n",
    "\n",
    "We now sample from the data with SRS and SSRS with optimal allocation. In practice, you would choose only strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs_evaluator = ModelPerformanceEvaluator(proxy_performance=proxy_performance, budget = budget)\n",
    "sample_indices_srs = srs_evaluator.sample_data(sampling_method = 'srs')\n",
    "\n",
    "# optimal allocation\n",
    "evaluator.allocate_budget(allocation_type=\"proportional\")\n",
    "sample_indices_ssrs = evaluator.sample_data(sampling_method=\"ssrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Annotate\n",
    "\n",
    "Pretend that in this step we annotate the selected samples. Here they are (luckily) already available in the torch file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = (predicted_labels == np.array(df['Target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Estimate\n",
    "\n",
    "Now we can estimate the performance on our data subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRS-HT</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.002428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRS-DF</th>\n",
       "      <td>0.415192</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSRS-opt-HT</th>\n",
       "      <td>0.454791</td>\n",
       "      <td>0.002146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             estimate  variance\n",
       "SRS-HT       0.420000  0.002428\n",
       "SRS-DF       0.415192  0.002000\n",
       "SSRS-opt-HT  0.454791  0.002146"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "estimates = pd.DataFrame({\n",
    "    'SRS-HT': srs_evaluator.compute_estimate(performance[sample_indices_srs], estimator=\"ht\"), \n",
    "    'SRS-DF': srs_evaluator.compute_estimate(performance[sample_indices_srs], estimator=\"df\"), \n",
    "    'SSRS-opt-HT': evaluator.compute_estimate(performance[sample_indices_ssrs], estimator=\"ht\")\n",
    "})\n",
    "\n",
    "estimates.index = ['estimate', 'variance']\n",
    "estimates.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "error-estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
